# Create a local GPU powered inference from a LLaMa model.

## Usage

Run `install.sh`. If you have a GPU on your machine, use the `--gpu` flag to install the correct dependencies for `llama-cpp`. For instance, to install dependencies with GPU support:

1. `chmod +x ./install.sh`
2. `./install.sh`

## Resources

1. [Running `llama-cpp` inferences on an Nvidia GPU](https://kubito.dev/posts/llama-cpp-linux-nvidia/)