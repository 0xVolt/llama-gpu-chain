# Create a local GPU powered inference from a LLaMa model.

## Resources

1. [Running `llama-cpp` inferences on an Nvidia GPU](https://kubito.dev/posts/llama-cpp-linux-nvidia/)
2. 