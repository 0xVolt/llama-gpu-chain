{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0xVolt/llama-gpu-chain/blob/main/llama_cpp_gpu_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vm6k-BdCmqa"
      },
      "source": [
        "# LLaMa GPU inference with `llama-cpp` and `cuBLAS`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Xz_1P4S81KX"
      },
      "source": [
        "### References\n",
        "\n",
        "1. [YouTube video on GPU inferences](https://www.youtube.com/watch?v=iLBekSpVFq4)\n",
        "2. [GitHub repository with code for GPU inference](https://github.com/MuhammadMoinFaisal/LargeLanguageModelsProjects/blob/main/Run%20Llama2%20Google%20Colab/Llama_2_updated.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4l7hyYhIEia"
      },
      "source": [
        "## Import and download dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgA-mePVCPzq"
      },
      "outputs": [],
      "source": [
        "%pip install huggingface_hub transformers sentencepiece\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --verbose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fD5cV40bEHXW"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_ElBpgEINQ7"
      },
      "source": [
        "## Download model from `huggingface_hub`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPcFxCkoCvUV"
      },
      "outputs": [],
      "source": [
        "checkpoint = \"TheBloke/CodeLlama-13B-Instruct-GGUF\"\n",
        "fileName = r\"codellama-13b-instruct.Q4_K_M.gguf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-D-PYCddEO6I"
      },
      "outputs": [],
      "source": [
        "modelPath = hf_hub_download(\n",
        "    repo_id=checkpoint,\n",
        "    filename=fileName\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMYDnIaP9bZu"
      },
      "source": [
        "Here's where the model was downloaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kXvj07D9bZu"
      },
      "outputs": [],
      "source": [
        "modelPath"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqOApc3eIVri"
      },
      "source": [
        "## Load downloaded model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7gcBTRpHtk3"
      },
      "outputs": [],
      "source": [
        "llm = Llama(\n",
        "    model_path=modelPath,\n",
        "    n_threads=2,\n",
        "    n_batch=512,\n",
        "    n_gpu_layers=28,\n",
        "    # n_ctx=3584,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGnl64LmJM0i"
      },
      "source": [
        "#### Things to note:\n",
        "\n",
        "1. `n_threads` - refers to your CPU cores\n",
        "2. `n_batches` - needs to be between 1 and `n_ctx`, i.e., the number of characters in the context window. Consider this param when tweaking the code to optimize GPU usage. **Look at how much VRAM you have!**\n",
        "3. `n_gpu_layers` - change this according to the GPU you're using and how much VRAM is has\n",
        "4. If you notice that `BLAS = 1`, this means that `llama-cpp` has setup properly with a GPU backend. In this case, we're usung `cuBLAS` to run inference on an Nvidia GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-2aHjlO9bZv"
      },
      "outputs": [],
      "source": [
        "# with open(r\"./test/testScript1.py\", \"r\") as file:\n",
        "#     function = file.read()\n",
        "\n",
        "function = \"\"\"\n",
        "def checkGPU(tensorflow):\n",
        "    if tensorflow == True:\n",
        "        import tensorflow as tf\n",
        "        print(\"Number of GPUs available with tensorflow:\", len(tf.config.list_physical_devices('GPU')))\n",
        "    else:\n",
        "        import torch\n",
        "        print('Checking if the GPU is available with PyTorch:', torch.cuda.is_available())\n",
        "\"\"\"\n",
        "\n",
        "function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXlkBSeV9bZv"
      },
      "outputs": [],
      "source": [
        "# prompt = f\"\"\"Here's my function in Python:\n",
        "\n",
        "# {function}\n",
        "\n",
        "# Given the definition of a function in Python, generate it's documentation. I want it complete with fields like function name, function arguments and return values as well as a detailed explanation of how the function logic works line-by-line. Make it concise and informative to put the documentation into a project.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6NerDOC9bZv"
      },
      "outputs": [],
      "source": [
        "prompt = f'''SYSTEM: You are a helpful, respectful and honest assistant. With every line of code that you read, try to understand it and explain it's working. Split the documentation into fields such as function name, function description, arguments, return values and line-by-line explanation. Output should be in markdown syntax.\n",
        "\n",
        "\n",
        "USER: Write this function's documentation:\\n{function}\n",
        "\n",
        "ASSISTANT:\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wm1bQTtR9bZw"
      },
      "outputs": [],
      "source": [
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8qvXJL09bZw"
      },
      "outputs": [],
      "source": [
        "response = llm(\n",
        "    prompt=prompt,\n",
        "    max_tokens=2048,\n",
        "    temperature=0.4,\n",
        "    top_p=0.95,\n",
        "    top_k=150,\n",
        "    repeat_penalty=1.2,\n",
        "    echo=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtrlsz5Y9bZw"
      },
      "outputs": [],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Oht9Jov9bZw"
      },
      "outputs": [],
      "source": [
        "print(json.dumps(response, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nv4bmAfa9bZw"
      },
      "outputs": [],
      "source": [
        "print(response[\"choices\"][0][\"text\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}